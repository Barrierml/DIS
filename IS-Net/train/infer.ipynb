{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fea4876d-06de-4c97-ab3e-0dbd2d7fadef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/DIS/IS-Net/train\n"
     ]
    }
   ],
   "source": [
    "%cd /root/DIS/IS-Net/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f23bb7d-25b1-49e4-9fc4-5c4ff4bccb57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "# project imports\n",
    "from data_loader_cache import normalize, im_reader, im_preprocess \n",
    "from isnet import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f003b21-17d8-47e7-828d-65f2409e161d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class GOSNormalize(object):\n",
    "    '''\n",
    "    Normalize the Image using torch.transforms\n",
    "    '''\n",
    "    def __init__(self, mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self,image):\n",
    "        image = normalize(image,self.mean,self.std)\n",
    "        return image\n",
    "\n",
    "\n",
    "transform =  transforms.Compose([GOSNormalize([0.5,0.5,0.5],[1.0,1.0,1.0])])\n",
    "\n",
    "def load_image(im_path, hypar):\n",
    "    if im_path.startswith(\"http\"):\n",
    "        im_path = BytesIO(requests.get(im_path).content)\n",
    "\n",
    "    im = im_reader(im_path)\n",
    "    im, im_shp = im_preprocess(im, hypar[\"cache_size\"])\n",
    "    im = torch.divide(im,255.0)\n",
    "    shape = torch.from_numpy(np.array(im_shp))\n",
    "    return transform(im).unsqueeze(0), shape.unsqueeze(0) # make a batch of image, shape\n",
    "\n",
    "\n",
    "def build_model(hypar,device):\n",
    "    net = hypar[\"model\"]#GOSNETINC(3,1)\n",
    "\n",
    "    # convert to half precision\n",
    "    if(hypar[\"model_digit\"]==\"half\"):\n",
    "        net.half()\n",
    "        for layer in net.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.float()\n",
    "\n",
    "    net.to(device)\n",
    "\n",
    "    if(hypar[\"restore_model\"]!=\"\"):\n",
    "        net.load_state_dict(torch.load(hypar[\"model_path\"]+\"/\"+hypar[\"restore_model\"],map_location=device))\n",
    "        net.to(device)\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "    \n",
    "def predict(net,  inputs_val, shapes_val, hypar, device):\n",
    "    '''\n",
    "    Given an Image, predict the mask\n",
    "    '''\n",
    "    net.eval()\n",
    "\n",
    "    if(hypar[\"model_digit\"]==\"full\"):\n",
    "        inputs_val = inputs_val.type(torch.FloatTensor)\n",
    "    else:\n",
    "        inputs_val = inputs_val.type(torch.HalfTensor)\n",
    "\n",
    "  \n",
    "    inputs_val_v = Variable(inputs_val, requires_grad=False).to(device) # wrap inputs in Variable\n",
    "   \n",
    "    ds_val = net(inputs_val_v)[0] # list of 6 results\n",
    "\n",
    "    pred_val = ds_val[0][0,:,:,:] # B x 1 x H x W    # we want the first one which is the most accurate prediction\n",
    "\n",
    "    ## recover the prediction spatial size to the orignal image size\n",
    "    pred_val = torch.squeeze(F.upsample(torch.unsqueeze(pred_val,0),(shapes_val[0][0],shapes_val[0][1]),mode='bilinear'))\n",
    "\n",
    "    ma = torch.max(pred_val)\n",
    "    mi = torch.min(pred_val)\n",
    "    pred_val = (pred_val-mi)/(ma-mi) # max = 1\n",
    "\n",
    "    if device == 'cuda': torch.cuda.empty_cache()\n",
    "    return (pred_val.detach().cpu().numpy()*255).astype(np.uint8) # it is the mask we need\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32c17a70-b9dc-42ce-8b4f-d015960a96e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hypar = {} # paramters for inferencing\n",
    "\n",
    "\n",
    "hypar[\"model_path\"] =\"/root/DIS/IS-Net/train/results/training_cloth_segm_dig/checkpoints\" ## load trained weights from this path\n",
    "hypar[\"restore_model\"] = \"itr_00024000_dis.pth\" ## name of the to-be-loaded weights\n",
    "hypar[\"interm_sup\"] = False ## indicate if activate intermediate feature supervision\n",
    "\n",
    "##  choose floating point accuracy --\n",
    "hypar[\"model_digit\"] = \"full\" ## indicates \"half\" or \"full\" accuracy of float number\n",
    "hypar[\"seed\"] = 0\n",
    "\n",
    "hypar[\"cache_size\"] = [768, 768] ## cached input spatial resolution, can be configured into different size\n",
    "\n",
    "## data augmentation parameters ---\n",
    "hypar[\"input_size\"] = [768, 768] ## mdoel input spatial size, usually use the same value hypar[\"cache_size\"], which means we don't further resize the images\n",
    "hypar[\"crop_size\"] = [768, 768] ## random crop size from the input, it is usually set as smaller than hypar[\"cache_size\"], e.g., [920,920] for data augmentation\n",
    "\n",
    "hypar[\"model\"] = ISNetDIS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "735b8492-f534-499b-b80b-0caffa0c5497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ISNetDIS:\n\tsize mismatch for side1.weight: copying a param with shape torch.Size([4, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for side1.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side2.weight: copying a param with shape torch.Size([4, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for side2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side3.weight: copying a param with shape torch.Size([4, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3]).\n\tsize mismatch for side3.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side4.weight: copying a param with shape torch.Size([4, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 256, 3, 3]).\n\tsize mismatch for side4.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side5.weight: copying a param with shape torch.Size([4, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 512, 3, 3]).\n\tsize mismatch for side5.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side6.weight: copying a param with shape torch.Size([4, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 512, 3, 3]).\n\tsize mismatch for side6.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for outconv.weight: copying a param with shape torch.Size([4, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 6, 1, 1]).\n\tsize mismatch for outconv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(hypar, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(hypar[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestore_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mhypar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrestore_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ISNetDIS:\n\tsize mismatch for side1.weight: copying a param with shape torch.Size([4, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for side1.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side2.weight: copying a param with shape torch.Size([4, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for side2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side3.weight: copying a param with shape torch.Size([4, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 128, 3, 3]).\n\tsize mismatch for side3.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side4.weight: copying a param with shape torch.Size([4, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 256, 3, 3]).\n\tsize mismatch for side4.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side5.weight: copying a param with shape torch.Size([4, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 512, 3, 3]).\n\tsize mismatch for side5.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for side6.weight: copying a param with shape torch.Size([4, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 512, 3, 3]).\n\tsize mismatch for side6.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).\n\tsize mismatch for outconv.weight: copying a param with shape torch.Size([4, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 6, 1, 1]).\n\tsize mismatch for outconv.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "net = build_model(hypar, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dc1d8-6753-4301-a614-bada59d31f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"https://aiyongaizjk.oss-cn-zhangjiakou.aliyuncs.com/datasets/child/paint/1581.png\"\n",
    "image_bytes = BytesIO(requests.get(image_path).content)\n",
    "\n",
    "image_tensor, orig_size = load_image(image_path, hypar) \n",
    "m0,m1,m2,m3,m4,m5,m6 = predict(net,image_tensor,orig_size, hypar, device)\n",
    "\n",
    "mask = m0\n",
    "\n",
    "f, ax = plt.subplots(1,2, figsize = (35,20))\n",
    "img_array = np.array(Image.open(image_bytes))\n",
    "\n",
    "# nnp = np.concatenate((img_array, mask[:, :, np.newaxis]), axis=2)\n",
    "nnp = img_array * (mask > 110)[:, :, np.newaxis]\n",
    "\n",
    "ax[0].imshow(img_array) # Original image\n",
    "ax[1].imshow(nnp) # retouched image\n",
    "\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[1].set_title(\"Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98f9c6-b877-40fe-98bf-31ba020dbc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
